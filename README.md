# Synthetically Expressive  
**Evaluating Gesture and Voice for Emotion and Empathy in VR and 2D Scenarios**  
üìç *Accepted at [ACM IVA 2025](https://iva.acm.org/2025/)*

üé• [Watch the project video](https://youtu.be/WMfjIB1X-dc?si=JmZD-4bLhxI2FbwM)

## Overview

The creation of virtual humans increasingly leverages automated synthesis of speech and gestures, enabling expressive, adaptable agents that effectively engage users. However, the independent development of voice and gesture generation technologies, alongside the growing popularity of virtual reality (VR), presents significant questions about the integration of these signals and their ability to convey emotional detail in immersive environments. In this paper, we evaluate the influence of real and synthetic gestures and speech, alongside varying levels of immersion (VR vs. 2D displays) and emotional contexts (positive, neutral, negative) on user perceptions. We investigate how immersion affects the perceived match between gestures and speech and the impact on key aspects of user experience, including emotional and empathetic responses and the sense of co-presence. Our findings indicate that while VR enhances the perception of natural gesture‚Äìvoice pairings, it does not similarly improve synthetic ones‚Äîamplifying the perceptual gap between them. These results highlight the need to reassess gesture appropriateness and refine AI-driven synthesis for immersive environments.

## Datasets and Tools

We use the following public datasets and frameworks:

- ü§ñ **[AMUSE](https://amuse.is.tue.mpg.de/)** ‚Äî an emotional speech-driven gesture synthesis model using latent diffusion ([Chhatre et al., CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/html/Chhatre_Emotional_Speech-driven_3D_Body_Animation_via_Disentangled_Latent_Diffusion_CVPR_2024_paper.html)).
- üé≠ **[BEAT Dataset](https://pantomatrix.github.io/BEAT/)** ‚Äî a large-scale motion-capture dataset for training and evaluating co-speech gesture generation systems ([Liu et al., ECCV 2022](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670605.pdf)).

These resources supported the creation of natural and synthetic gesture-speech combinations used in the study.

## Funding

This work was conducted with the financial support of the **Research Ireland Centre for Research Training in Digitally-Enhanced Reality (d-real)** under Grant No. **18/CRT/6224**.

For the purpose of Open Access, the author has applied a **CC BY public copyright licence** to any Author Accepted Manuscript version arising from this submission.


